{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pothuraju1690/pothuraju-2000/blob/main/Mod5_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x9mXnIyH_TU"
      },
      "source": [
        "# Lab 2\n",
        "# Classification II : Introduction to Decision Trees\n",
        "\n",
        "```\n",
        "Module Coordinator : Akshit Garg\n",
        "\n",
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of some property by inferring simple decision rules from the data features.\n",
        "\n",
        "\n",
        "Let us take a look at an example of a decision tree which predicts the class of the species of Iris flower from the iris dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRLg2DwJTqMN"
      },
      "source": [
        "#Importing the necessary packages\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8eLU5YOhiPH"
      },
      "source": [
        "### Code for the core experiment:\n",
        "\n",
        "- Creating the decision tree classifier based on parameters passed.\n",
        "- Evaluating the classifier's accuracy and plotting its confusion matrix.\n",
        "- Plotting its decision boundary.\n",
        "- Creating and showing the visualization of the tree made.\n",
        "\n",
        "**SKIP THE CODE IN THE FOLLOWING CELL FOR NOW AND COME BACK TO IT LATER AFTER UNDERSTANDING THE IDEA AND INTUITION BEHIND DECISION TREES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNo2x3Tbhhu3"
      },
      "source": [
        "def performExperiment(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  #Importing the Decision tree classifier from sklearn:\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))\n",
        "\n",
        "  print(\"The confusion matrix is : \")\n",
        "  plot_confusion_matrix(clf, X_test, y_test, display_labels=class_names)\n",
        "\n",
        "\n",
        "  print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "  fig, ax = plt.subplots(figsize=(12,10))\n",
        "  imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 12)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def giveAnExample(n : int):\n",
        "  performExperiment((X_train, y_train),  (X_test, y_test), feature_names = iris[\"feature_names\"], class_names = iris[\"target_names\"], max_depth = n)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  plt.title(\"Decision Boundary for two features used in Decision Tree\")\n",
        "  # plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsxUs3oYbFNK"
      },
      "source": [
        "## Loading IRIS Dataset:\n",
        "\n",
        "### About the IRIS dataset:\n",
        "\n",
        "The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). We shall be using decision trees to try to predict the correct species of the flower using these four features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VR4gNQ5Vuwk"
      },
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "irisData = pandas.DataFrame(\\\n",
        "    data = np.hstack((X,y.reshape(y.shape[0], 1), [[iris[\"target_names\"][int(classIdx)]] for classIdx in y])), \\\n",
        "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', \"Class\", \"ClassName\"])\n",
        "irisData.sample(n = 10, random_state = 1)\n",
        "\n",
        "#Here is a few samples: The dataset has 4 non-catagorical features and a class which can take of one of the three values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oN3nv1ExEcJ"
      },
      "source": [
        "## Example of DT on Iris dataset with performace evaluation, and tree structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxwk-zEuqc9I"
      },
      "source": [
        "giveAnExample(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkrm75u1x9RX"
      },
      "source": [
        "### Exercise 1:\n",
        " Kindly use the above tree to evaluate the classes for the following examples and verify what percent of them are classified correctly by the tree:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31oDgYlw2cg"
      },
      "source": [
        "irisData.sample(n = 5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjTv9DSA1zbg"
      },
      "source": [
        "Now let us see how we perform when we try to have a more complex decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKqh4_BzB4K"
      },
      "source": [
        "giveAnExample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7wSLX5zTIr"
      },
      "source": [
        "### Exercise 2:\n",
        "Repeat Exercise 1 for the above tree as well.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We observe that even though that the tree had four features available to it, the tree uses only two of them to classify the cases of species. It gives us an idea that those two features chosen are performing quite decently. Let us examine the decision boundary generated by the tree when only those two features namely **petal length and petal width** are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJN66PO-4Q0h"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [2, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [2, 3], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQ6VO7J78vp"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKnuGzgf6eoZ"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fVoKPWL8W-B"
      },
      "source": [
        "**Decision boundary** with considering **sepal length and pedal length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSDC6Il7wDA"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 2]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKiQd5Y8ox5"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and pedal width**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6StxQVn8i01"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [1, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uuefe-k_dAO"
      },
      "source": [
        "---\n",
        "\n",
        "### Exercise 3:\n",
        "\n",
        "#### 3.1 :\n",
        "We see that the above decision boundaries are with depth of 3. Compare the above boundary with trees that have higher complexity (by changing the value of `max_depth`) and then pause and ponder.\n",
        "\n",
        "Test with `max_depth` of the following values:\n",
        "- 2\n",
        "- 5\n",
        "- 10\n",
        "\n",
        "\n",
        "What do you observe?\n",
        "\n",
        "#### 3.2 :\n",
        "\n",
        "On a closer look, we see that the decision boundaries' lines are always at a right angle to the principle axes. Can you reason on why is that the case? \\\n",
        "`(Hint: How is a decision made at any node?)`\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4:\n",
        "\n",
        "Complete the following function predict: which takes in four variables : `sepal width, sepal length, petal width, petal length` and returns the class of the flower. Use the decision tree made in Exercise 2 and realise the logic using multiple nested `if else` statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAIeeEFY84oA"
      },
      "source": [
        "def predictSpecies(sepal_width, sepal_length, petal_width,  petal_length) -> str :\n",
        "  \"\"\"\n",
        "    Write your program here to return the species of the plant (string) using if else statements.\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 5)\n",
        "pair = [2, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "metadata": {
        "id": "NqoP3HUELk2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 2)\n",
        "pair = [1, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "metadata": {
        "id": "4gfaIePOLoR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 10)\n",
        "pair = [1, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "metadata": {
        "id": "_t9XiEKKLwSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "clf.fit(X, y)\n",
        "\n",
        "feature_names = iris[\"feature_names\"]\n",
        "class_names = iris[\"target_names\"]\n",
        "print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "fig, ax = plt.subplots(figsize=(12,10))\n",
        "imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xfzFcqKWLzuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictSpecies(sepal_width, sepal_length, petal_width,  petal_length) -> str :\n",
        "  \"\"\"\n",
        "    Write your program here to return the species of the plant (string) using if else statements.\n",
        "  \"\"\"\n",
        "  if petal_width <= 0.8:\n",
        "    return \"sentosa\"\n",
        "  else:\n",
        "    if petal_width <= 1.75:\n",
        "      if petal_length <= 4.95:\n",
        "        return \"versicolor\"\n",
        "      else:\n",
        "        return \"verginica\"\n",
        "    else:\n",
        "      if petal_length <= 4.85:\n",
        "        return \"verginica\"\n",
        "      else:\n",
        "        return \"verginica\""
      ],
      "metadata": {
        "id": "apOtSm7pL2RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4agDMEMCNeB-"
      },
      "source": [
        "# Entropy and Information:\n",
        "\n",
        "## How are decision trees built?\n",
        "\n",
        "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).\n",
        "We use entropy to calculate the homogeneity of a sample.\n",
        "\n",
        "Entropy itself is defined in the following way:\n",
        "\n",
        "$$E(s) = \\sum_{i=1}^c - p_i * log_2(p_i)$$\n",
        "\n",
        "Where $i$ iterates through the classes of the current group and $p_i$ is the probability of choosing an item from class $i$ when a datapoint is randomly picked from the group.\n",
        "\n",
        "At anypoint in the process of making the decision tree. All possible methods of dividing the group are considered (across all features and values of separations) and then the division with the most amount of **Information Gain** is used to divide the current group into two. This is done recursively to finally attain a tree.\n",
        "\n",
        "Here Information Gain is defined by the difference in Entropy of the group before the division and the weighted sum of the entropy of the two groups after division.\n",
        "\n",
        "$$IG(X) = E(s) - E(s, X)$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnd4qhrM1Km"
      },
      "source": [
        "irisData.sample(n = 10, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMDVOKZRY1GE"
      },
      "source": [
        "## Exercise 5:\n",
        "Calculate the Entropy of the above collection of 10 datapoints.\n",
        "## Exercise 6:\n",
        "Suggest a decision node (if, else) statement which divides the group into two groups. Also compute the Information Gain in that division step. Compare this with other decision clauses that you can make and intuitively comment on which is better for classification and observe if this has any correlation with the numerical value of Information Gain.\n",
        "\n",
        "---\n",
        "\n",
        "End of Lab 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Excercise 5***"
      ],
      "metadata": {
        "id": "-dp8qKFOMBAK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTrqJ86cYcFL"
      },
      "source": [
        "X = irisData.sample(n = 10, random_state = 5)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = X[\"Class\"]\n",
        "entropy = 0\n",
        "total = y.value_counts().sum()\n",
        "for count in y.value_counts():\n",
        "  p_i = count / total\n",
        "  entropy += -p_i * np.log2(p_i)\n",
        "\n",
        "print(\"Entropy of the first {total} item is {entropy:.3f}\".format(total=total, entropy=entropy))\n"
      ],
      "metadata": {
        "id": "JafIM8UpMNCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***EXERCISE 6***"
      ],
      "metadata": {
        "id": "QiX6KAhrMOyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_information_gain(irisData, split_name, target_name):\n",
        "    \"\"\"\n",
        "    Calculate information gain given a data set, column to split on, and target\n",
        "    \"\"\"\n",
        "    # Calculate the original entropy\n",
        "    original_entropy = calc_entropy(irisData[target_name])\n",
        "\n",
        "    #Find the unique values in the column\n",
        "    values = irisData[split_name].unique()\n",
        "\n",
        "\n",
        "    # Make two subsets of the data, based on the unique values\n",
        "    left_split = irisData[irisData[split_name] == values[0]]\n",
        "    right_split = irisData[irisData[split_name] == values[1]]\n",
        "\n",
        "    # Loop through the splits and calculate the subset entropies\n",
        "    to_subtract = 0\n",
        "    for subset in [left_split, right_split]:\n",
        "        prob = (subset.shape[0] / irisData.shape[0])\n",
        "        to_subtract += prob * calc_entropy(subset[target_name])\n",
        "\n",
        "    # Return information gain\n",
        "    return original_entropy - to_subtract\n"
      ],
      "metadata": {
        "id": "y_byfX9_MUle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width','Class']"
      ],
      "metadata": {
        "id": "ruRB4oBFMaVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def highest_info_gain(columns):\n",
        "  #Intialize an empty dictionary for information gains\n",
        "  information_gains = {}\n",
        "\n",
        "  #Iterate through each column name in our list\n",
        "  for col in columns:\n",
        "    #Find the information gain for the column\n",
        "    information_gain = calc_information_gain(irisData, sepal_width, 'ClassName')\n",
        "    #Add the information gain to our dictionary using the column name as the ekey\n",
        "    information_gains[col] = information_gain\n",
        "\n",
        "  #Return the key with the highest value\n",
        "  return max(information_gains, key=information_gains.get)\n",
        "\n"
      ],
      "metadata": {
        "id": "9sfYh_BXMdA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}